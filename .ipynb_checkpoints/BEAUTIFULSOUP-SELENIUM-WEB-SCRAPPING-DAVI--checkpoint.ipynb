{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf360bb9-de94-4dba-8f30-af20ebe45a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.10'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Exibir versão do Python\n",
    "import platform\n",
    "import logging\n",
    "import pandas as pd\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9b1009-dc47-424b-acf7-65fede0621d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Checando se Scrapy está instalado\n",
    "    import scrapy\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6908e633-a33c-475a-b123-1870f85f6ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('ataques.json', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4126f-0081-4f84-9ac8-72f37d4c4673",
   "metadata": {},
   "source": [
    "### ***Classe do Objeto Ataque Separados por Item Especificos para Pesquisar na Página Crawleada***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187acb39-e9eb-4a90-a54a-1042c35e7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem1(scrapy.Item):\n",
    "    #categorias = scrapy.Field()\n",
    "    categoria01 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "def get_categoria1(response):\n",
    "    categoria1 = response.css('div.middlecolumn option::text')[0].extract()\n",
    "\n",
    "    return categoria1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbea042-1073-4905-912f-a27f4e3d569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem2(scrapy.Item):\n",
    "    categoria02 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "def get_categoria2(response):\n",
    "    categoria2 = response.css('div.middlecolumn option::text')[1].extract()\n",
    "\n",
    "    return categoria2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e6b1b9-e6f6-4d05-a0d2-6cf1bcc5fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem3(scrapy.Item):\n",
    "    categoria03 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "def get_categoria3(response):\n",
    "    categoria3 = response.css('div.middlecolumn option::text')[2].extract()\n",
    "\n",
    "    return categoria3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0f22e9-cb5b-4281-b1b9-d15712067c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem4(scrapy.Item):\n",
    "    categoria04 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "    \n",
    "def get_categoria4(response):\n",
    "    categoria4 = response.css('div.middlecolumn option::text')[3].extract()\n",
    "\n",
    "    return categoria4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b15f3c09-f530-4e7a-a547-b97f77c21c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem5(scrapy.Item):\n",
    "    categoria05 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "def get_categoria5(response):\n",
    "    categoria5 = response.css('div.middlecolumn option::text')[4].extract()\n",
    "\n",
    "    return categoria5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "979bd098-e62e-4b8d-b8d0-3c18204939c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem6(scrapy.Item):\n",
    "    categoria06 = scrapy.Field()    \n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "def get_categoria6(response):\n",
    "    categoria6 = response.css('div.middlecolumn option::text')[5].extract()\n",
    "\n",
    "    return categoria6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a8d98b-02c8-4ce1-b142-be6980e5cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem7(scrapy.Item):\n",
    "    categoria07 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "def get_categoria7(response):\n",
    "    categoria7 = response.css('div.middlecolumn option::text')[6].extract()\n",
    "\n",
    "    return categoria7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33852dd8-79b6-4c0c-9fee-a59e710b07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem8(scrapy.Item):\n",
    "    categoria08 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "    \n",
    "def get_categoria8(response):\n",
    "    categoria8 = response.css('div.middlecolumn option::text')[7].extract()\n",
    "\n",
    "    return categoria8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b3cfd-146d-431b-870d-d2faa7851ecb",
   "metadata": {},
   "source": [
    "### ***Métodos de Captura de Informações do Objeto***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51afee40-bd28-4ebf-ace9-039320f4fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_categorias(response):\n",
    "    categorias = response.css('div.middlecolumn option::text').extract()\n",
    "           \n",
    "    categorias_list = []\n",
    "    \n",
    "    for i in categorias:\n",
    "        categorias_list.append(i.strip())\n",
    "\n",
    "    categ = ' '.join(categorias_list)\n",
    "    \n",
    "    return categ\n",
    "\n",
    "def get_periodo(response):\n",
    "    periodo = response.css('label ::text')[0].extract()\n",
    "        \n",
    "    return periodo\n",
    "\n",
    "\n",
    "def get_ataques(response):\n",
    "    ataques = response.css('')\n",
    "        \n",
    "    return ataques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f062cc-bb79-4598-a9d3-d5144ea8b4c3",
   "metadata": {},
   "source": [
    "# Paǵinas por Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc23ec41-d9ab-45dd-83c0-eeb2cf719a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat1Ataques(scrapy.Spider):\n",
    "    name = \"ataques\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques.jl'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items1 = AtaqueItem1()\n",
    "        items1['categoria01'] = get_categoria1(response)\n",
    "        items1['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60251156-7415-428a-a31a-938b63efc0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat2Ataques(scrapy.Spider):\n",
    "    name = \"ataques\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques.jl'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items2 = AtaqueItem2()\n",
    "        items2['categoria02'] = get_categoria2(response)\n",
    "        items2['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f72740cd-beca-4d2d-a481-acb06230db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat3Ataques(scrapy.Spider):\n",
    "    name = \"ataques\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques.jl'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items3 = AtaqueItem3()\n",
    "        items3['categoria03'] = get_categoria3(response)\n",
    "        items3['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a5ac6fe-9dfe-468e-87c4-577e33b19db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat4Ataques(scrapy.Spider):\n",
    "    name = \"ataques\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques.jl'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items4 = AtaqueItem4()\n",
    "        items4['categoria04'] = get_categoria4(response)\n",
    "        items4['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eddd3a9b-b0b5-4db6-b343-82191641a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat5Ataques(scrapy.Spider):\n",
    "    name = \"ataques\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques.jl'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items5 = AtaqueItem5()\n",
    "        items5['categoria05'] = get_categoria5(response)\n",
    "        items5['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd16cfa1-2d3b-4766-a6f7-7294d3fbdb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat6Ataques(scrapy.Spider):\n",
    "    name = \"ataques\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques.jl'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items6 = AtaqueItem6()\n",
    "        items6['categoria06'] = get_categoria6(response)\n",
    "        items6['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "211f573e-c000-4ac7-bae2-44f8a32a0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat7Ataques(scrapy.Spider):\n",
    "    name = \"ataques\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques.jl'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items7 = AtaqueItem7()\n",
    "        items7['categoria07'] = get_categoria7(response)\n",
    "        items7['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac1fcd7c-5d6d-4b59-ad3f-a37f0eaf273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat8Ataques(scrapy.Spider):\n",
    "    name = \"ataques\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques.jl'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items8 = AtaqueItem8()\n",
    "        items8['categoria08'] = get_categoria8(response)\n",
    "        items8['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dc5f074-9903-46f1-9b3f-7d619027cfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 23:23:09 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-11 23:23:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-11 23:23:09 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n",
      "2022-06-11 23:23:09 [py.warnings] WARNING: /home/olp_rsi/.local/lib/python3.8/site-packages/scrapy/extensions/feedexport.py:289: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7fe7d4dc9ac0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 23:23:14 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-11 23:23:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-11 23:23:14 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n",
      "2022-06-11 23:23:14 [py.warnings] WARNING: /home/olp_rsi/.local/lib/python3.8/site-packages/scrapy/extensions/feedexport.py:289: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7fe7d3d899a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 23:23:16 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-11 23:23:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-11 23:23:16 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7fe7d3d3b760>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 23:23:19 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-11 23:23:19 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-11 23:23:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7fe7d3d8dfd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 23:23:22 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-11 23:23:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-11 23:23:22 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7fe7d3d5bcd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 23:23:24 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-11 23:23:24 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-11 23:23:24 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7fe7d3d702b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 23:23:27 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-11 23:23:27 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-11 23:23:27 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7fe7d3d1fdc0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 23:23:29 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-11 23:23:29 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-11 23:23:29 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7fe7d3cfc2e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "\n",
    "# Iniciando processo\n",
    "process.crawl(Cat1Ataques)\n",
    "process.start()\n",
    "\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process2 = CrawlerProcess(get_project_settings())\n",
    "process2.crawl(Cat2Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process2.start()\n",
    "\n",
    "\n",
    "process3 = CrawlerProcess(get_project_settings())\n",
    "process3.crawl(Cat3Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process3.start()\n",
    "\n",
    "\n",
    "process4 = CrawlerProcess(get_project_settings())\n",
    "process4.crawl(Cat4Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process4.start()\n",
    "\n",
    "process5 = CrawlerProcess(get_project_settings())\n",
    "process5.crawl(Cat5Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process5.start()\n",
    "\n",
    "process6 = CrawlerProcess(get_project_settings())\n",
    "process6.crawl(Cat6Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process6.start()\n",
    "\n",
    "process7 = CrawlerProcess(get_project_settings())\n",
    "process7.crawl(Cat7Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process7.start()\n",
    "\n",
    "process8 = CrawlerProcess(get_project_settings())\n",
    "process8.crawl(Cat8Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process8.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cf73548-e99a-4f1c-9af5-ede379e4ba0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0a6e28598176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Carregando JSON criado para visualizar saída\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ataques.jl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mdata_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1133\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             )\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "# Carregando JSON criado para visualizar saída\n",
    "output = pd.read_json('ataques.jl', lines=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9459f4-c353-49db-9169-650734d40d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
