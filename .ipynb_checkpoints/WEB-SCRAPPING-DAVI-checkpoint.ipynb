{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf360bb9-de94-4dba-8f30-af20ebe45a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.10'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Exibir versão do Python\n",
    "import platform\n",
    "import logging\n",
    "import pandas as pd\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9b1009-dc47-424b-acf7-65fede0621d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Checando se Scrapy está instalado\n",
    "    import scrapy\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6908e633-a33c-475a-b123-1870f85f6ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonWriterPipeline1(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('ataques1.jl', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "\n",
    "class JsonWriterPipeline2(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('ataques2.jl', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "    \n",
    "class JsonWriterPipeline3(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('ataques3.jl', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "    \n",
    "class JsonWriterPipeline4(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('ataques4.jl', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "    \n",
    "class JsonWriterPipeline5(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('ataques5.jl', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "    \n",
    "class JsonWriterPipeline6(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('ataques6.jl', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "\n",
    "class JsonWriterPipeline7(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('ataques7.jl', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "    \n",
    "class JsonWriterPipeline8(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('ataques8.jl', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4126f-0081-4f84-9ac8-72f37d4c4673",
   "metadata": {},
   "source": [
    "### ***Classe do Objeto Ataque Separados por Item Especificos para Pesquisar na Página Crawleada***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187acb39-e9eb-4a90-a54a-1042c35e7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem1(scrapy.Item):\n",
    "    #categorias = scrapy.Field()\n",
    "    categoria01 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "def get_categoria1(response):\n",
    "    categoria1 = response.css('div.middlecolumn option::text')[0].extract()\n",
    "\n",
    "    return categoria1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbea042-1073-4905-912f-a27f4e3d569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem2(scrapy.Item):\n",
    "    categoria02 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "def get_categoria2(response):\n",
    "    categoria2 = response.css('div.middlecolumn option::text')[1].extract()\n",
    "\n",
    "    return categoria2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e6b1b9-e6f6-4d05-a0d2-6cf1bcc5fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem3(scrapy.Item):\n",
    "    categoria03 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "def get_categoria3(response):\n",
    "    categoria3 = response.css('div.middlecolumn option::text')[2].extract()\n",
    "\n",
    "    return categoria3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0f22e9-cb5b-4281-b1b9-d15712067c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem4(scrapy.Item):\n",
    "    categoria04 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "    \n",
    "def get_categoria4(response):\n",
    "    categoria4 = response.css('div.middlecolumn option::text')[3].extract()\n",
    "\n",
    "    return categoria4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b15f3c09-f530-4e7a-a547-b97f77c21c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem5(scrapy.Item):\n",
    "    categoria05 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "def get_categoria5(response):\n",
    "    categoria5 = response.css('div.middlecolumn option::text')[4].extract()\n",
    "\n",
    "    return categoria5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "979bd098-e62e-4b8d-b8d0-3c18204939c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem6(scrapy.Item):\n",
    "    categoria06 = scrapy.Field()    \n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "def get_categoria6(response):\n",
    "    categoria6 = response.css('div.middlecolumn option::text')[5].extract()\n",
    "\n",
    "    return categoria6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a8d98b-02c8-4ce1-b142-be6980e5cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem7(scrapy.Item):\n",
    "    categoria07 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "def get_categoria7(response):\n",
    "    categoria7 = response.css('div.middlecolumn option::text')[6].extract()\n",
    "\n",
    "    return categoria7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33852dd8-79b6-4c0c-9fee-a59e710b07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtaqueItem8(scrapy.Item):\n",
    "    categoria08 = scrapy.Field()\n",
    "    periodo = scrapy.Field()\n",
    "    ataques = scrapy.Field()\n",
    "    \n",
    "    \n",
    "def get_categoria8(response):\n",
    "    categoria8 = response.css('div.middlecolumn option::text')[7].extract()\n",
    "\n",
    "    return categoria8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b3cfd-146d-431b-870d-d2faa7851ecb",
   "metadata": {},
   "source": [
    "### ***Métodos de Captura de Informações do Objeto***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51afee40-bd28-4ebf-ace9-039320f4fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_categorias(response):\n",
    "    categorias = response.css('div.middlecolumn option::text').extract()\n",
    "           \n",
    "    categorias_list = []\n",
    "    \n",
    "    for i in categorias:\n",
    "        categorias_list.append(i.strip())\n",
    "\n",
    "    categ = ' '.join(categorias_list)\n",
    "    \n",
    "    return categ\n",
    "\n",
    "def get_periodo(response):\n",
    "    periodo = response.css('label ::text')[0].extract()\n",
    "        \n",
    "    return periodo\n",
    "\n",
    "\n",
    "def get_ataques(response):\n",
    "    ataques = response.css('')\n",
    "        \n",
    "    return ataques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f062cc-bb79-4598-a9d3-d5144ea8b4c3",
   "metadata": {},
   "source": [
    "# Paǵinas por Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc23ec41-d9ab-45dd-83c0-eeb2cf719a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat1Ataques(scrapy.Spider):\n",
    "    name = \"ataque1\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline1': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques1.json'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items1 = AtaqueItem1()\n",
    "        items1['categoria01'] = get_categoria1(response)\n",
    "        items1['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60251156-7415-428a-a31a-938b63efc0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat2Ataques(scrapy.Spider):\n",
    "    name = \"ataque2\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline2': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques2.json'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items2 = AtaqueItem2()\n",
    "        items2['categoria02'] = get_categoria2(response)\n",
    "        items2['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f72740cd-beca-4d2d-a481-acb06230db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat3Ataques(scrapy.Spider):\n",
    "    name = \"ataque3\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline3': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques3.json'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items3 = AtaqueItem3()\n",
    "        items3['categoria03'] = get_categoria3(response)\n",
    "        items3['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a5ac6fe-9dfe-468e-87c4-577e33b19db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat4Ataques(scrapy.Spider):\n",
    "    name = \"ataque4\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline4': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques4.json'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items4 = AtaqueItem4()\n",
    "        items4['categoria04'] = get_categoria4(response)\n",
    "        items4['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eddd3a9b-b0b5-4db6-b343-82191641a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat5Ataques(scrapy.Spider):\n",
    "    name = \"ataque5\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline5': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques5.json'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items5 = AtaqueItem5()\n",
    "        items5['categoria05'] = get_categoria5(response)\n",
    "        items5['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd16cfa1-2d3b-4766-a6f7-7294d3fbdb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat6Ataques(scrapy.Spider):\n",
    "    name = \"ataque6\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline6': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques6.json'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items6 = AtaqueItem6()\n",
    "        items6['categoria06'] = get_categoria6(response)\n",
    "        items6['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "211f573e-c000-4ac7-bae2-44f8a32a0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat7Ataques(scrapy.Spider):\n",
    "    name = \"ataque7\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline7': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques7.json'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items7 = AtaqueItem7()\n",
    "        items7['categoria07'] = get_categoria7(response)\n",
    "        items7['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac1fcd7c-5d6d-4b59-ad3f-a37f0eaf273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat8Ataques(scrapy.Spider):\n",
    "    name = \"ataque8\"\n",
    "    \n",
    "    # Lista de Strings (URLs)\n",
    "    # Poderíamos ter uma lista de URLs\n",
    "    start_urls = [\n",
    "        'https://cybermap.kaspersky.com/pt/stats'\n",
    "    ]\n",
    "    \n",
    " # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline8': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'ataques8.json'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items8 = AtaqueItem8()\n",
    "        items8['categoria08'] = get_categoria8(response)\n",
    "        items8['periodo'] = get_periodo(response)\n",
    "        #items['ataques'] = get_ataques(response)\n",
    "\n",
    "        yield items8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dc5f074-9903-46f1-9b3f-7d619027cfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 15:38:46 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-14 15:38:46 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-14 15:38:46 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n",
      "2022-06-14 15:38:46 [py.warnings] WARNING: /home/olp_rsi/.local/lib/python3.8/site-packages/scrapy/extensions/feedexport.py:289: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7f8fa84a28b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 15:38:49 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-14 15:38:49 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-14 15:38:49 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n",
      "2022-06-14 15:38:49 [py.warnings] WARNING: /home/olp_rsi/.local/lib/python3.8/site-packages/scrapy/extensions/feedexport.py:289: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7f8fa7454d60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 15:38:51 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-14 15:38:51 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-14 15:38:51 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7f8fa746a310>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 15:38:54 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-14 15:38:54 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-14 15:38:54 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7f8fa7419c10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 15:38:56 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-14 15:38:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-14 15:38:56 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7f8fa85df040>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 15:38:58 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-14 15:38:58 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-14 15:38:58 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7f8fa73caee0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 15:39:00 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-14 15:39:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-14 15:39:00 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7f8fa73f5ee0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 15:39:02 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-14 15:39:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-117-generic-x86_64-with-glibc2.29\n",
      "2022-06-14 15:39:02 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7f8fa740da60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "\n",
    "# Iniciando processo\n",
    "process.crawl(Cat1Ataques)\n",
    "process.start()\n",
    "\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process2 = CrawlerProcess(get_project_settings())\n",
    "process2.crawl(Cat2Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process2.start()\n",
    "\n",
    "\n",
    "process3 = CrawlerProcess(get_project_settings())\n",
    "process3.crawl(Cat3Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process3.start()\n",
    "\n",
    "\n",
    "process4 = CrawlerProcess(get_project_settings())\n",
    "process4.crawl(Cat4Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process4.start()\n",
    "\n",
    "process5 = CrawlerProcess(get_project_settings())\n",
    "process5.crawl(Cat5Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process5.start()\n",
    "\n",
    "process6 = CrawlerProcess(get_project_settings())\n",
    "process6.crawl(Cat6Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process6.start()\n",
    "\n",
    "process7 = CrawlerProcess(get_project_settings())\n",
    "process7.crawl(Cat7Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process7.start()\n",
    "\n",
    "process8 = CrawlerProcess(get_project_settings())\n",
    "process8.crawl(Cat8Ataques)\n",
    "\n",
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "process8.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1cf73548-e99a-4f1c-9af5-ede379e4ba0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categoria01</th>\n",
       "      <th>periodo</th>\n",
       "      <th>categoria02</th>\n",
       "      <th>categoria03</th>\n",
       "      <th>categoria04</th>\n",
       "      <th>categoria05</th>\n",
       "      <th>categoria06</th>\n",
       "      <th>categoria07</th>\n",
       "      <th>categoria08</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On-Access Scan</td>\n",
       "      <td>Última semana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Última semana</td>\n",
       "      <td>On Demand Scanner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Última semana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mail Anti-Virus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Última semana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web Anti-Virus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Última semana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Intrusion Detection Scan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Última semana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vulnerability Scan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Última semana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kaspersky Anti-Spam</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Última semana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Botnet Activity Detection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      categoria01        periodo        categoria02      categoria03  \\\n",
       "0  On-Access Scan  Última semana                NaN              NaN   \n",
       "1             NaN  Última semana  On Demand Scanner              NaN   \n",
       "2             NaN  Última semana                NaN  Mail Anti-Virus   \n",
       "3             NaN  Última semana                NaN              NaN   \n",
       "4             NaN  Última semana                NaN              NaN   \n",
       "5             NaN  Última semana                NaN              NaN   \n",
       "6             NaN  Última semana                NaN              NaN   \n",
       "7             NaN  Última semana                NaN              NaN   \n",
       "\n",
       "      categoria04               categoria05         categoria06  \\\n",
       "0             NaN                       NaN                 NaN   \n",
       "1             NaN                       NaN                 NaN   \n",
       "2             NaN                       NaN                 NaN   \n",
       "3  Web Anti-Virus                       NaN                 NaN   \n",
       "4             NaN  Intrusion Detection Scan                 NaN   \n",
       "5             NaN                       NaN  Vulnerability Scan   \n",
       "6             NaN                       NaN                 NaN   \n",
       "7             NaN                       NaN                 NaN   \n",
       "\n",
       "           categoria07                categoria08  \n",
       "0                  NaN                        NaN  \n",
       "1                  NaN                        NaN  \n",
       "2                  NaN                        NaN  \n",
       "3                  NaN                        NaN  \n",
       "4                  NaN                        NaN  \n",
       "5                  NaN                        NaN  \n",
       "6  Kaspersky Anti-Spam                        NaN  \n",
       "7                  NaN  Botnet Activity Detection  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregando JSON criado para visualizar saída\n",
    "#pd.read_json('ataques2.json') => comando para ler os arquivos '.json' no outro formato não carrega-os \n",
    "\n",
    "ataques1 = pd.read_json('ataques1.json')\n",
    "ataques2 = pd.read_json('ataques2.json')\n",
    "ataques3 = pd.read_json('ataques3.json')\n",
    "ataques4 = pd.read_json('ataques4.json')\n",
    "ataques5 = pd.read_json('ataques5.json')\n",
    "ataques6 = pd.read_json('ataques6.json')\n",
    "ataques7 = pd.read_json('ataques7.json')\n",
    "ataques8 = pd.read_json('ataques8.json')\n",
    "\n",
    "\n",
    "ataques1c = pd.concat((ataques1, ataques2), ignore_index=True)\n",
    "ataques2c = pd.concat((ataques1c, ataques3), ignore_index=True)\n",
    "ataques3c = pd.concat((ataques2c, ataques4), ignore_index=True)\n",
    "ataques4c = pd.concat((ataques3c, ataques5), ignore_index=True)\n",
    "ataques5c = pd.concat((ataques4c, ataques6), ignore_index=True)\n",
    "ataques6c = pd.concat((ataques5c, ataques7), ignore_index=True)\n",
    "ataques7c = pd.concat((ataques6c, ataques8), ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "ataques7c\n",
    "# ataques2.T\n",
    "# ataques3.T\n",
    "# ataques4.T\n",
    "# ataques5.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9459f4-c353-49db-9169-650734d40d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
